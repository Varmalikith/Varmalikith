{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the csv file from the azure blobs with in the date range and insert into data base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import datetime\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "account_name = \"daywisebilling\"\n",
    "STORAGEACCOUNTURL = f\"https://{account_name}.blob.core.windows.net\"\n",
    "STORAGEACCOUNTKEY = \"jaCG7aC1Ilp7vpe9xDko8eMwqMTMiFNyY+ArzFTTkoutRQGkAt7KqkeB1HOX4d7cWyKKatRLLIxI+AStcecHHA==\"\n",
    "CONTAINERNAME = \"day-wise-billing\"\n",
    "BLOBNAME_PREFIX = \"dev/DailyBilling/\"\n",
    "# Define the storage account name and access key\n",
    "account_name = \"daywisebilling\"\n",
    "account_key = \"jaCG7aC1Ilp7vpe9xDko8eMwqMTMiFNyY+ArzFTTkoutRQGkAt7KqkeB1HOX4d7cWyKKatRLLIxI+AStcecHHA==\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found latest updated blob: dev/DailyBilling/20230101-20230131/DailyBilling_8b5b095a-5afe-4810-bbb8-04843145a473.csv\n"
     ]
    }
   ],
   "source": [
    "def extract_date_from_blob_name(blob_name):\n",
    "    # Extract the date from the blob name in the format \"20230101-20230131\"\n",
    "    date_str = blob_name.split(\"/\")[-2]\n",
    "    return date_str\n",
    "\n",
    "# Create a BlobServiceClient using the account URL and credentials\n",
    "blob_service_client = BlobServiceClient(account_url=STORAGEACCOUNTURL, credential=account_key)\n",
    "\n",
    "# Get a reference to the desired container\n",
    "container_client = blob_service_client.get_container_client(CONTAINERNAME)\n",
    "\n",
    "# Get a list of all blobs in the container\n",
    "blobs = container_client.list_blobs()\n",
    "\n",
    "start_date = \"20230101\"\n",
    "end_date = \"20230131\"\n",
    "\n",
    "# Create an empty list to store the matching blobs\n",
    "matching_blobs = []\n",
    "\n",
    "# Iterate through the list of blobs and find the ones that match the date range\n",
    "for blob in blobs:\n",
    "    date_str = extract_date_from_blob_name(blob.name)\n",
    "    if start_date <= date_str <= end_date and \".csv\" in blob.name:\n",
    "        # Add the matching blob to the list\n",
    "        matching_blobs.append(blob)\n",
    "\n",
    "# Find the latest updated blob from the matching list\n",
    "latest_blob = max(matching_blobs, key=lambda x: x.last_modified)\n",
    "\n",
    "# Get a reference to the latest updated blob\n",
    "blob_client = container_client.get_blob_client(latest_blob.name)\n",
    "\n",
    "print(f\"Found latest updated blob: {latest_blob.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_columns():\n",
    "    connection = psycopg2.connect(\n",
    "    database=\"postgres\",\n",
    "    user=\"postgres\",\n",
    "    password=\"Cescost#123\",\n",
    "    host=\"ces-cost-opt-01.c9girek67w2d.us-east-1.rds.amazonaws.com\",\n",
    "    port='5432'\n",
    ")\n",
    "    try:\n",
    "        cursor = connection.cursor()\n",
    "        Query = \"SELECT column_name FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME = 'azure_data';\"\n",
    "        cursor.execute(Query)\n",
    "        records = cursor.fetchall()\n",
    "        # connection.commit()\n",
    "        columns = []\n",
    "        for i in records:\n",
    "            columns.append(i[0])\n",
    "        # columns=columns\n",
    "        return columns\n",
    "    except:\n",
    "        print(\"check whether the database exits or not\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is Inserted\n"
     ]
    }
   ],
   "source": [
    "conn = psycopg2.connect(\n",
    "    database=\"postgres\",\n",
    "    user=\"postgres\",\n",
    "    password=\"Cescost#123\",\n",
    "    host=\"ces-cost-opt-01.c9girek67w2d.us-east-1.rds.amazonaws.com\",\n",
    "    port='5432'\n",
    ")\n",
    "# columns = get_columns()\n",
    "cursor = conn.cursor()\n",
    "stream = container_client.get_blob_client(latest_blob).download_blob()\n",
    "df = pd.read_csv(StringIO(stream.content_as_text()))\n",
    "df.columns=map(str.lower,df.columns)\n",
    "# print(df)\n",
    "buffer = StringIO()\n",
    "columns = get_columns()\n",
    "df = df[columns]\n",
    "\n",
    "df.to_csv(buffer, index = False ,sep=\"\\t\",header = False)\n",
    "buffer.seek(0)\n",
    "\n",
    "cursor.copy_from(buffer, \"azure_data\", sep=\"\\t\", null=\"\")\n",
    "print(\"Data is Inserted\")\n",
    "# print(stream)\n",
    "conn.commit()\n",
    "\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checks the condition if the data is already inserted or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the database\n",
    "conn = psycopg2.connect(\n",
    "    database=\"postgres\",\n",
    "    user=\"postgres\",\n",
    "    password=\"Cescost#123\",\n",
    "    host=\"ces-cost-opt-01.c9girek67w2d.us-east-1.rds.amazonaws.com\",\n",
    "    port='5432'\n",
    ")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Get the latest CSV file\n",
    "stream = container_client.get_blob_client(latest_blob).download_blob()\n",
    "df = pd.read_csv(StringIO(stream.content_as_text()))                                                  #Checks the condition if the date ia already inserted or not\n",
    "df.columns=map(str.lower,df.columns)\n",
    "\n",
    "# Get the dates that are already present in the database\n",
    "query = \"SELECT usagedatetime FROM azure_data\"\n",
    "cursor.execute(query)\n",
    "existing_dates = [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "# Filter the data in the CSV file to only include the dates that are not present in the database\n",
    "df = df[~df['usagedatetime'].isin(existing_dates)]\n",
    "\n",
    "# Insert the filtered data into the database\n",
    "buffer = StringIO()\n",
    "columns = get_columns()\n",
    "df = df[columns]\n",
    "df.to_csv(buffer, index = False ,sep=\"\\t\",header = False)\n",
    "buffer.seek(0)\n",
    "cursor.copy_from(buffer, \"azure_data\", sep=\"\\t\", null=\"\")\n",
    "print(\"Data is Inserted\")\n",
    "conn.commit()\n",
    "\n",
    "# Close the database connection\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download te latest updated csv file from every month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20230101\n",
      "20230101\n",
      "20230201\n",
      "20230201\n"
     ]
    }
   ],
   "source": [
    "import calendar,os\n",
    "\n",
    "# Create a BlobServiceClient using the account URL and credentials\n",
    "blob_service_client = BlobServiceClient(account_url=STORAGEACCOUNTURL, credential=account_key)\n",
    "\n",
    "# Get a reference to the desired container\n",
    "container_client = blob_service_client.get_container_client(CONTAINERNAME)\n",
    "\n",
    "local_directory = r\"C:\\\\Users\\\\p.likith varma\\\\Desktop\\\\billing_data\"\n",
    "\n",
    "def extract_date_from_blob_path(blob_path):\n",
    "    date_str = blob_path.split(\"/\")[-2]\n",
    "    start_date, end_date = date_str.split(\"-\")\n",
    "    return start_date, end_date\n",
    "                                                                         # Download the latest updated file from every month!!!!!!!!!!!!!!!\n",
    "blobs = container_client.list_blobs()\n",
    "\n",
    "# Keep track of the latest updated file for each month\n",
    "latest_files = {}\n",
    "\n",
    "for blob in blobs:\n",
    "    start_date_blob, end_date_blob = extract_date_from_blob_path(blob.name)\n",
    "    start_month = start_date_blob[:6]\n",
    "    end_month = end_date_blob[:6]\n",
    "\n",
    "    if start_month not in latest_files:\n",
    "        latest_files[start_month] = None\n",
    "\n",
    "    # Check if the blob is a CSV file and if it's the latest for its month\n",
    "    if \".csv\" in blob.name and (not latest_files[start_month] or blob.last_modified > latest_files[start_month].last_modified):\n",
    "        latest_files[start_month] = blob\n",
    "\n",
    "for start_month, blob in latest_files.items():\n",
    "    start_date = start_month + \"01\"\n",
    "    end_date = start_month + str(calendar.monthrange(int(start_month[:4]), int(start_month[4:]))[1])\n",
    "    print(start_date)\n",
    "\n",
    "    # Get a reference to the latest updated CSV file for this month\n",
    "    blob_client = container_client.get_blob_client(blob.name)\n",
    "    print(start_date)\n",
    "    # Download the contents of the file\n",
    "    local_file_path = os.path.join(local_directory, os.path.basename(blob.name))\n",
    "    with open(local_file_path, \"wb\") as f:\n",
    "        f.write(blob_client.download_blob().readall())\n",
    "\n",
    "    # Do something with the file contents...\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download latest file of everymonth and insert into the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar,os\n",
    "\n",
    "# Create a BlobServiceClient using the account URL and credentials\n",
    "blob_service_client = BlobServiceClient(account_url=STORAGEACCOUNTURL, credential=account_key)\n",
    "\n",
    "# Get a reference to the desired container\n",
    "container_client = blob_service_client.get_container_client(CONTAINERNAME)\n",
    "\n",
    "local_directory = r\"C:\\\\Users\\\\p.likith varma\\\\Desktop\\\\billing_data\"\n",
    "\n",
    "def extract_date_from_blob_path(blob_path):\n",
    "    date_str = blob_path.split(\"/\")[-2]\n",
    "    start_date, end_date = date_str.split(\"-\")\n",
    "    return start_date, end_date\n",
    "                                                                         # Download the latest updated file from every month!!!!!!!!!!!!!!!\n",
    "blobs = container_client.list_blobs()\n",
    "\n",
    "# Keep track of the latest updated file for each month\n",
    "latest_files = {}\n",
    "\n",
    "for blob in blobs:\n",
    "    start_date_blob, end_date_blob = extract_date_from_blob_path(blob.name)\n",
    "    start_month = start_date_blob[:6]\n",
    "    end_month = end_date_blob[:6]\n",
    "\n",
    "    if start_month not in latest_files:\n",
    "        latest_files[start_month] = None\n",
    "\n",
    "    # Check if the blob is a CSV file and if it's the latest for its month\n",
    "    if \".csv\" in blob.name and (not latest_files[start_month] or blob.last_modified > latest_files[start_month].last_modified):\n",
    "        latest_files[start_month] = blob\n",
    "\n",
    "for start_month, blob in latest_files.items():\n",
    "    start_date = start_month + \"01\"\n",
    "    end_date = start_month + str(calendar.monthrange(int(start_month[:4]), int(start_month[4:]))[1])\n",
    "    print(start_date)\n",
    "\n",
    "    # Get a reference to the latest updated CSV file for this month\n",
    "    blob_client = container_client.get_blob_client(blob.name)\n",
    "    \n",
    "    print(start_date)\n",
    "    # Download the contents of the file\n",
    "    # local_file_path = os.path.join(local_directory, os.path.basename(blob.name))\n",
    "    # with open(local_file_path, \"wb\") as f:\n",
    "    #     f.write(blob_client.download_blob().readall())\n",
    "    \n",
    "    conn = psycopg2.connect(\n",
    "    database=\"postgres\",\n",
    "    user=\"postgres\",\n",
    "    password=\"Cescost#123\",\n",
    "    host=\"ces-cost-opt-01.c9girek67w2d.us-east-1.rds.amazonaws.com\",\n",
    "    port='5432'\n",
    ")\n",
    "# columns = get_columns()\n",
    "    cursor = conn.cursor()\n",
    "    stream = container_client.get_blob_client(blob.name).download_blob()\n",
    "    df = pd.read_csv(StringIO(stream.content_as_text()))\n",
    "    df.columns=map(str.lower,df.columns)\n",
    "    # print(df)\n",
    "    buffer = StringIO()\n",
    "    columns = get_columns()\n",
    "    df = df[columns]\n",
    "\n",
    "    df.to_csv(buffer, index = False ,sep=\"\\t\",header = False)\n",
    "    buffer.seek(0)\n",
    "\n",
    "    cursor.copy_from(buffer, \"azure_data\", sep=\"\\t\", null=\"\")\n",
    "    print(\"Data is Inserted\")\n",
    "    # print(stream)\n",
    "    conn.commit()\n",
    "\n",
    "  \n",
    "\n",
    "    # Do something with the file contents...\n",
    "def get_columns():\n",
    "    connection = psycopg2.connect(\n",
    "    database=\"postgres\",\n",
    "    user=\"postgres\",\n",
    "    password=\"Cescost#123\",\n",
    "    host=\"ces-cost-opt-01.c9girek67w2d.us-east-1.rds.amazonaws.com\",\n",
    "    port='5432'\n",
    ")\n",
    "    try:\n",
    "        cursor = connection.cursor()\n",
    "        Query = \"SELECT column_name FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME = 'azure_data';\"\n",
    "        cursor.execute(Query)\n",
    "        records = cursor.fetchall()\n",
    "        # connection.commit()\n",
    "        columns = []\n",
    "        for i in records:\n",
    "            columns.append(i[0])\n",
    "        # columns=columns\n",
    "        return columns\n",
    "    except:\n",
    "        print(\"check whether the database exits or not\")\n",
    "        \n",
    "\n",
    "cursor.close()\n",
    "conn.close()  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download latest csv file from every month and import it into database and check wheather the file is existing or not\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20230101\n",
      "20230131\n",
      "Data is Inserted\n",
      "20230201\n",
      "20230228\n",
      "Data is Inserted\n"
     ]
    }
   ],
   "source": [
    "import calendar,os\n",
    "\n",
    "# Create a BlobServiceClient using the account URL and credentials\n",
    "blob_service_client = BlobServiceClient(account_url=STORAGEACCOUNTURL, credential=account_key)\n",
    "\n",
    "# Get a reference to the desired container\n",
    "container_client = blob_service_client.get_container_client(CONTAINERNAME)\n",
    "\n",
    "local_directory = r\"C:\\\\Users\\\\p.likith varma\\\\Desktop\\\\billing_data\"\n",
    "\n",
    "def extract_date_from_blob_path(blob_path):\n",
    "    date_str = blob_path.split(\"/\")[-2]\n",
    "    start_date, end_date = date_str.split(\"-\")\n",
    "    return start_date, end_date\n",
    "                                                                         # Download the latest updated file from every month!!!!!!!!!!!!!!!\n",
    "blobs = container_client.list_blobs()\n",
    "\n",
    "# Keep track of the latest updated file for each month\n",
    "latest_files = {}\n",
    "\n",
    "for blob in blobs:\n",
    "    start_date_blob, end_date_blob = extract_date_from_blob_path(blob.name)\n",
    "    # print(start_date_blob)\n",
    "    # print(end_date_blob)\n",
    "    start_month = start_date_blob[:6]\n",
    "    end_month = end_date_blob[:6]\n",
    "\n",
    "    if start_month not in latest_files:\n",
    "        latest_files[start_month] = None\n",
    "\n",
    "    # Check if the blob is a CSV file and if it's the latest for its month\n",
    "    if \".csv\" in blob.name and (not latest_files[start_month] or blob.last_modified > latest_files[start_month].last_modified):\n",
    "        latest_files[start_month] = blob    \n",
    "\n",
    "for start_month, blob in latest_files.items():\n",
    "    start_date = start_month + \"01\"\n",
    "    end_date = start_month + str(calendar.monthrange(int(start_month[:4]), int(start_month[4:]))[1])\n",
    "    print(start_date)\n",
    "    print(end_date)\n",
    "    # Get a reference to the latest updated CSV file for this month\n",
    "    blob_client = container_client.get_blob_client(blob.name)\n",
    "    \n",
    "    # print(start_date)\n",
    "    # Download the contents of the file\n",
    "    # local_file_path = os.path.join(local_directory, os.path.basename(blob.name))\n",
    "    # with open(local_file_path, \"wb\") as f:\n",
    "    #     f.write(blob_client.download_blob().readall())\n",
    "    \n",
    "    conn = psycopg2.connect(\n",
    "    database=\"postgres\",\n",
    "    user=\"postgres\",\n",
    "    password=\"Cescost#123\",\n",
    "    host=\"ces-cost-opt-01.c9girek67w2d.us-east-1.rds.amazonaws.com\",\n",
    "    port='5432'\n",
    ")\n",
    "# columns = get_columns()\n",
    "    cursor = conn.cursor()\n",
    "    stream = container_client.get_blob_client(blob.name).download_blob()\n",
    "    df = pd.read_csv(StringIO(stream.content_as_text()))\n",
    "    df.columns=map(str.lower,df.columns)\n",
    "    \n",
    "    # Get the dates that are already present in the database\n",
    "    query = \"SELECT usagedatetime FROM azure_data\"\n",
    "    cursor.execute(query)\n",
    "    existing_dates = [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "# Filter the data in the CSV file to only include the dates that are not present in the database\n",
    "    df = df[~df['usagedatetime'].isin(existing_dates)]\n",
    "    # print(df)\n",
    "    buffer = StringIO()\n",
    "    # columns = get_columns()\n",
    "    # df = df[columns]\n",
    "\n",
    "    df.to_csv(buffer, index = False ,sep=\"\\t\",header = False)\n",
    "    buffer.seek(0)\n",
    "\n",
    "    cursor.copy_from(buffer, \"azure_data\", sep=\"\\t\", null=\"\")\n",
    "    print(\"Data is Inserted\")\n",
    "    # print(stream)\n",
    "    conn.commit()\n",
    "\n",
    "  \n",
    "\n",
    "    # Do something with the file contents...\n",
    "# def get_columns():\n",
    "#     connection = psycopg2.connect(\n",
    "#     database=\"postgres\",\n",
    "#     user=\"postgres\",\n",
    "#     password=\"Cescost#123\",\n",
    "#     host=\"ces-cost-opt-01.c9girek67w2d.us-east-1.rds.amazonaws.com\",\n",
    "#     port='5432'\n",
    "# )\n",
    "#     try:\n",
    "#         cursor = connection.cursor()\n",
    "#         Query = \"SELECT column_name FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME = 'azure_data';\"\n",
    "#         cursor.execute(Query)\n",
    "#         records = cursor.fetchall()\n",
    "#         # connection.commit()\n",
    "#         columns = []\n",
    "#         for i in records:\n",
    "#             columns.append(i[0])\n",
    "#         # columns=columns\n",
    "#         return columns\n",
    "#     except:\n",
    "#         print(\"check whether the database exits or not\")\n",
    "        \n",
    "\n",
    "cursor.close()\n",
    "conn.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "921edd2f82bfb1b861c9fb56897f73163b38ec064156b76f5989eb6671381db3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
